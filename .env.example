# ==================== LLM Provider Configuration ====================
# Choose your LLM provider: ollama, deepseek, or openai
LLM_PROVIDER=ollama

# ==================== Ollama Configuration (Local) ====================
# Ollama base URL (local deployment)
OLLAMA_BASE_URL=http://localhost:11434/v1

# Ollama API key (leave empty for local)
OLLAMA_API_KEY=

# Model names
OLLAMA_MODEL=qwen2.5:7b
OLLAMA_VISION_MODEL=qwen2.5vl:32b

# ==================== DeepSeek Configuration (Cloud) ====================
# Get your API key from: https://platform.deepseek.com
# DEEPSEEK_API_KEY=your_api_key_here
# DEEPSEEK_MODEL=deepseek-chat
# DEEPSEEK_VISION_MODEL=deepseek-vl

# ==================== OpenAI Configuration (Cloud) ====================
# Get your API key from: https://platform.openai.com
# OPENAI_API_KEY=your_api_key_here
# OPENAI_MODEL=gpt-4
# OPENAI_VISION_MODEL=gpt-4-vision-preview

# ==================== Vision Model Configuration ====================
# Enable vision model for image analysis
USE_VISION_MODEL=true

# Vision model provider (same as LLM_PROVIDER usually)
VISION_MODEL_PROVIDER=ollama

# ==================== Browser Configuration ====================
# Run browser in headless mode (no GUI)
BROWSER_HEADLESS=true

# Browser timeout in milliseconds
BROWSER_TIMEOUT=30000

# ==================== Agent Configuration ====================
# Temperature for LLM (0.0 = deterministic, 2.0 = creative)
TEMPERATURE=0.1

# Maximum iterations for agent loop
MAX_ITERATIONS=20
